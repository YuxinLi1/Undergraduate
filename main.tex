\documentclass{ecnuthesis}
% \documentclass[printMode]{ecnuthesis}
% 模版选项:
% printMode     是否开启打印模式, 若缺省则为关闭, 反之则为开启
% 用法示例
% \documentclass[printMode]{ecnuthesis}   (开启打印模式, 适合双面打印)
% \documentclass[printMode]{ecnuthesis}   (关闭打印模式, 适合提交电子版)


% \setCJKfamilyfont{simsun}{SimSun.ttf}
% \newcommand{\SimSun}{\CJKfamily{simsun}}

%% myprotocol
\usepackage[ruled,linesnumbered,vlined]{algorithm2e}
\usepackage[noend]{algorithmic}

\newenvironment{myprotocol}{
    \hrule
    \smallskip
    % \scriptsize
    % \algsetup{linenosize=\tiny}
    \begin{algorithmic}[1]
        \newcommand{\SPACE}{\item[]}
        \newcommand{\GETS}{:=}
        \newcommand{\TITLE}[2]{\item[] \textbf{\underline{##1}} (##2) \textbf{:}\\[0.5pt]}
        \makeatletter
            \newcommand{\EVENT}[1]{\STATE \textbf{event} ##1 \textbf{do}\begin{ALC@g}}
            \newcommand{\ENDEVENT}{\end{ALC@g}}
        \makeatother
}{
    \end{algorithmic}
    \smallskip
    \hrule
}

\ecnuSetup {
  % 参数设置
  % 允许采用两种方式设置选项:
  %   1. style/... = ...
  %   2. style = { ... = ... } 
  % 注意事项: 
  %   1. 请勿在参数设置中出现空行
  %   2. "=" 两侧的空格将被忽略
  %   3. "/" 两侧的空格不会被忽略
  %   4. 请使用英文逗号 "," 分隔选项
  %
  % info 类用于输入论文信息
  info = {
    title = {基于拜占庭容错的地理分布共识系统实现},
    % 中文标题
    %
    titleEN = {Implementation of a Global Scale Consensus System Based on Byzantine Fault Tolerance},
    % 英文标题
    %
    % 如果名字包含生僻字导致输出错误，请注释掉line34，取消注释line33，将名字输入在“包含生僻字的名字”，字体获取见README.md
    % author = {\CJKfontspec{SimSun.ttf}[AutoFakeBold = {3.17}]{包含生僻字的名字}},
    author = {李昱鑫},
    % 作者姓名
    %
    studentID = {10174507132},
    % 作者学号
    %
    department = {数据科学与工程学院},
    % 学院名称
    %
    major = {数据科学与大数据技术},
    % 专业名称
    %
    supervisor = {张召},
    % 指导教师姓名
    %
    academicTitle = {教授},
    % 指导教师职称
    %
    year  = 2022,
    % 论文完成年份
    %
    month = 5,
    % 论文完成月份
    %
    keywords = {区块链，共识算法，拜占庭容错，地理分布},
    % 中文关键词
    % 请使用英文逗号 "," 以分隔
    %
    keywordsEN = {blockchain, consensus algorithm, Byzantine fault tolerance, geographic distribution, global scale},
    % 英文关键词
    % 请使用英文逗号 "," 以分隔
    %
  },
  % style 类用于简单设置论文格式
  style = {
    footnote  = plain,
    % 脚注编号样式
    % 可用选项:
    %   footnote = plain|circled
    % 说明:
    %   plain     脚注的编号仅为数字
    %   circled   脚注的编号为带圆圈数字 (仅限1-10)
    %   (默认选项为 plain )
    %
    numbering = chinese,
    % 章节编号样式
    % 可用选项:
    %   numbering = arabic|alpha|chinese
    % 说明:
    %   arabic    使用数字进行编号 (即理科要求)
    %   alpha     使用字母进行编号 (即外文要求)
    %   chinese   使用汉字进行编号 (即文科要求)
    %   (默认选项为 arabic )
    %
    fontCJK = fandol,
    % 中文字体选择
    % 可用选项:
    %   fontCJK = fandol|windows|mac|default
    % 说明:
    %   fandol    使用 TeX 自带的 fandol 字体
    %   windows   使用 Windows 系统内的字体 (中易)
    %   mac       使用 MacOS 系统内的字体
    %   (默认选项为 fandol )
    %
    bibResource = {thesis-ref.bib},
    % 参考文献数据源
    % 由于使用的是 biber + biblatex , 所以必须明确给出 .bib 后缀名
    %
    logoResource = {./source/inner-cover(contains_font).eps},
    % 封面插图数据源
    % 模版已自带, 默认选项也已经设置为 ./source/inner-cover(contains_font).eps
  }
}



% 需要的宏包可以自行调用
\usepackage{mwe}
\usepackage{booktabs}
\usepackage{pgfplotstable}
\usepackage{tikz}
% \usepackage[ruled,linesnumbered,vlined]{algorithm2e}

\begin{document}

% 设置前置部分编号
\frontmatter

% 中文摘要环境
\begin{abstract}
  
随着区块链网络中副本规模的不断增大，并且在跨地理范围内分布，现有的拜占庭容错算法，如实用拜占庭容错共识协议PBFT等，其通信复杂度高，不能很好的支持跨地理范围部署副本。因此，需要对能够支持大规模跨地理范围部署副本的共识协议进行研究。地理分布共识算法通过利用网络拓扑信息将副本进行分组，在本地集群引入共识并行化，以及通过最小化集群全局间通信，实现了良好的可扩展性。通过实现地理分布共识算法并进行测量分析，得出以下结果：随着集群数量的增加，地理分布共识算法实现了良好的可扩展性；随着集群中副本数量的增加，尽管地理分布共识算法性能有所下降，但仍然优于实用拜占庭容错共识算法；随着批大小——单次共识决策中处理的客户端交易的数量增加，地理分布共识算法能够支持更大的批。通过结果表明，地理分布共识算法在多个集群的多个副本上分散共识，消除了由于任何单个主副本或集群的带宽和延迟造成的瓶颈，能够支持大规模跨地理范围部署副本。
\end{abstract}

% 英文摘要环境
\begin{abstractEN}
  
The scale of replicas in blockchain networks is constantly increasing, and replicas are deployed on a global scale. Unfortunately, existing Byzantine fault-tolerant consensus protocols, such as Practical Byzantine Fault-tolerant consensus protocol PBFT, are not designed to deal with geo-scale deployments in which many replicas spread across a geographically large area participate in consensus. Therefore, research is needed on consensus protocols that can support large-scale deployment of replicas across geographic scales. Geo-distributed Byzantine fault-tolerant consensus algorithms GeoBFT achieve good scalability by grouping replicas using network topology information, introducing consensus parallelism in local clusters, and by minimizing global inter-cluster communication. By implementing GeoBFT and conducting measurement analysis, the following results are obtained: First, as the number of clusters increases, the GeoBFT achieves good scalability. Second, as the number of replicas in the cluster increases, although the performance of GeoBFT decreases, it still outperforms PBFT. Third, as batch size - the number of client transactions processed in a single consensus decision - increases, GeoBFT is able to support larger batches.The results show that the geographically distributed consensus algorithm decentralizes consensus across multiple replicas of multiple clusters, eliminates bottlenecks due to bandwidth and latency of any single master replica or cluster, and is able to support large-scale global deployment of replicas.
\end{abstractEN}

% 设置正文编号
\mainmatter

\chapter{绪论}
\section{研究背景}

共识协议是区块链系统的核心模块，直接决定了区块链在其分布式系统特性上的安全性与活性，目前学术界提出的共识协议主要分为四类：通过竞争记账权间接达成共识的PoX协议（PoW\cite{dwork1992pricing}\cite{jakobsson1999proofs}，PoS\cite{king2012ppcoin}等），通过投票直接达成共识的BFT协议（PBFT\cite{castro1999practical}\cite{li2020scalable}、HotStuff\cite{yin2018hotstuff}\cite{yin2019hotstuff}等），通过选举代理人间接参与共识的代理协议以及舍弃传统线性链组织结构的DAG协议（SharPer\cite{amiri2021sharper}）。

在网络中节点规模较小时，现有的共识算法通常能时线较高的吞吐量和较低的时延。但随着网络中节点规模增大，并且这些节点分布在全球不同的地理位置，继续采用现有的共识算法系统的性能便会快速下降。相关测量结果表明，全局跨域网络的时延比本地局域网高33-270倍，全局跨域网络的吞吐量比本地局域网低10-151倍\cite{gupta2020resilientdb}，而现有的共识算法并不能感知这种地理分布导致的性能差异。

因此，如果要部署一个跨地域的区块链系统，其共识协议的设计必须要考虑地域分布特性，并能够感知出本地通信和全局通信，从而实现系统的最佳性能。除此之外，主流的共识协议如支持拜占庭容错的实用拜占庭容错协议PBFT，其设计是集中式的，通过单一主节点协调所有的一致性决策，这与区块链去中心化的理念相悖，并且通信复杂度与节点数量的平方成正比\cite{cite_label4}，扩展性较差。而其它的共识协议如联邦拜占庭协议FBA、HotStuff等同样具有中心化的领导者节点，也无法识别本地通信和跨域通信。

为了处理以上协议存在的问题，地理分布的共识协议需要考虑网络集群拓扑关系，识别出局域间通信和全局间通信。同时，也需要保证去中心化特性：任何单一节点或集群都不应该独自主导和协调全局的一致性共识决策——集中式设计会将全局带宽吞吐量和网络延迟限制在该单一节点或网络集群上。因此，具有地理分布感知的共识系统能够有效的提升在全球范围内部署的区块链系统的性能，减少网络延迟并有效提升带宽。

\section{研究内容与方法}

在实用拜占庭容错共识协议PBFT的基础上进行优化和改良，通过使集群中的副本节点能够感知网络拓扑关系，将在地理上接近的副本节点划分到同一个集群内，将副本之间两两通信改为集群内副本间通信和全局上的集群间通信，从而将PBFT中由单一主节点协调的集中式共识决策改为去中心化的共识决策。

首先，需要实现单一集群内部达成共识决策的实用拜占庭容错共识协议PBFT。其次，需要实现支持集群间通信的优化全局共享协议。然后，为了支持优化的全局共享协议，需要实现一个新的远程视图更换（view change）协议，用于处理不同于单集群且更为复杂的主节点不响应或崩溃问题。最后，单个副本节点可以对接收到的已确定的交易进行排序与执行。

\section{组织结构}

本文第二节介绍了系统的需求分析和系统的基本流程；第三节对即与拜占庭容错的地理分布共识算法进行了叙述与证明；第四节对系统的网络场景和构建进行了详细描述；第五节给出了实验结果与分析，最后总结全文并展望。

% 新增：调整了itemize的间距
% 调整了itemize的间距
% \begin{itemize}
%     \item 第1部分为绪论部分。
%     \item 第2部分主要对...
%     \item 第3部分为模型设计与实现，是本研究工作的核心。该部分详述了实验设计、环境、参数选择以及其他细节。
%     \item ...
% \end{itemize}

\chapter{系统分析}

\section{需求分析}

区块链系统的核心共识算法都是拜占庭容错共识算法，它可以在系统中存在某些故障或恶意副本节点时提供活性与安全性保障。现有的区块链系统通常采用基于传统拜占庭容错共识的许可区块链设计\cite{herlihy2019blockchains}\cite{ozsu1999principles}\cite{tel2000introduction}。这些许可区块链采用全复制模式，其中的每个副本都是已知的，并且每个副本都保存有所有链上数据的完整拷贝。相关测量结果表明，区域之间的通信比区域内部的通信成本高几个数量级，为了使许可区块链能够在跨地理范围内实现高性能部署副本，必须要区分本地通信和全局通信，并最小化全局通信。

因此，在跨地理范围共识协议中，必须要包含两个重要特性。其一是能够感知网络拓扑关系，并将处于同一个区域中的副本聚集在一起，并在这些副本内进行本地通信而不是全局通信。其二是分散共识，任何单一副本节点或集群都不应该负责协调所有的一致性决策，集中式设计会将吞吐量限制在主副本或单个集群的带宽和网络时延。而现有流行的共识算法不同时具备这两个条件。

在表\ref{tab:复杂度分析表}中，给出了跨地理分布共识算法和几种流行的拜占庭容错共识算法的复杂度分析对比\cite{gupta2020resilientdb}\cite{cite_label1}。

\begin{table}[h]
    \centering
    \bicaption{几种共识算法的通信复杂度（$\mathbf{z}$个集群，每个集群$\mathbf{n}$个副本）}{Communication complexity of several consensus algorithms (z clusters, n replicas per cluster)}
    \begin{tabular}{lcccc}
    \toprule
        协议 & 决策数量 & \multicolumn{2}{c}{通信复杂度} & 中心化 \\
        & & \textit{$($本地$)$} & \textit{$($全局$)$} & \\
        \hline
        地理分布共识算法 & $\mathbf{z}$ & $\mathcal{O}(\mathbf{zn}^2)$ & $\mathcal{O}(\mathbf{fz}^2)$ & 去中心化 \\
        \multicolumn{1}{r}{\rotatebox[c]{180}{$\Lsh$}单次决策} & 1 & $\mathcal{O}(\mathbf{n}^2)$ & $\mathcal{O}(\mathbf{fz})$ & 去中心化 \\
        Steward & 1 & $\mathcal{O}(2\mathbf{zn}^2)$ & $\mathcal{O}(\mathbf{z}^2)$ & 去中心化 \\
        Pbft & 1 & \multicolumn{2}{c}{$\mathcal{O}(2(\mathbf{zn})^2)$} & 中心化 \\
        Zyzzyva & 1 & \multicolumn{2}{c}{$\mathcal{O}(\mathbf{zn})$} & 中心化 \\
        PoE & 1 & \multicolumn{2}{c}{$\mathcal{O}((\mathbf{zn})^2)$} & 中心化 \\
        HotStuff & 1 & \multicolumn{2}{c}{$\mathcal{O}(8(\mathbf{zn}))$} & 半去中心化 \\
    \bottomrule
    \end{tabular}
    \label{tab:复杂度分析表}
\end{table}

\section{系统概要}

跨地理分布共识算法将一个区域中的所有副本集中到一个集群内，并且让每个集群独立的做出一致性决策。然后通过全局共享协议以最小通信与其他的集群共享一致性决策，从而保证所有的集群能够得到相同的一致性决策。同样，也将每个客户端分配给其所在区域的集群。这种集群划分有助于在跨地理规模部署副本节点中实现高吞吐量和良好的可扩展性。

在每一轮共识决策中，每个集群都能够对所属该集群的客户端请求达成一致性决策，并在通过全局共享阶段后对整个系统的该轮次的所有客户端请求达成共识。跨地理分布共识算法的每一轮次由图\ref{figure:steps}中的三个步骤组成：本地复制、全局共享以及排序执行，将在下面进行详细介绍。

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.8]{source/steps.png}
    \bicaption{\centering 一轮地理分布共识算法的步骤}{Steps in a round of GeoBFT}
    \label{figure:steps}
    % 如果中文/英文太长，可以使用 \centering 命令来控制其位置
\end{figure}

\section{系统流程}

首先在每一轮的开始，每个集群选择本地客户端的一个请求。然后每个集群使用实用拜占庭容错共识协议Pbft本地复制该事务。在本地复制阶段结束时，Pbft保证每个无故障副本都可以通过生成提交证明以证明本地复制成功。

在全局共享阶段，每个集群都会与其他所有集群共享本地复制阶段的客户端请求和生成的提交证明，然后在本地集群内转发接收到的全局共享阶段消息。为了处理故障，需要使用一种新的远程视图更换协议。

在接收到其他所有集群本地复制阶段的客户端请求后，每个集群中的每个副本都可以确定性地对这些请求进行排序并执行。执行完成后，每个集群中的副本仅向本地客户端回复其请求的执行结果。

在图\ref{figure:geobft}中描述了由两个集群，每个集群4个副本组成的系统中一个轮次的地理分布共识算法。

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{source/geobft.png}
    \bicaption{\centering 一轮地理分布共识算法的步骤}{Steps in a round of GeoBFT}
    \label{figure:geobft}
    % 如果中文/英文太长，可以使用 \centering 命令来控制其位置
\end{figure}

\chapter{算法分析}

\section{概要}

\begin{table}[h]
    \centering
    \bicaption{符号表1}{Symbol table 1}
    \begin{tabular}{cc}
    \toprule
        符号 & 定义 \\
        \hline
        $R$ & 副本 \\
        $C$ & 集群，$C = \{ R_1, \cdots , R_n \}$ \\
        $S$ & 系统，$S = \{ C_1, \cdots , C_z \}$ \\
        $\mathbf{n}$ & 集群中的副本数量 \\
        $\mathbf{z}$ & 系统中的集群数量 \\
        $\mathbf{f}$ & 单个集群中最大容错数 \\
        $f(C_i)$ & 集群$C_i$中的无故障副本 \\
        $nf(C_i)$ & 集群$C_i$中的故障副本 \\
        $P_C$ & 集群$C$的主节点 \\
        $\langle m \rangle_u$ & 表示消息$m$由$u$签名\\
    \bottomrule
    \end{tabular}
    \label{tab:符号表}
\end{table}

为了描述地理分布共识算法，过程中使用的符号如表\ref{tab:符号表}中所示。

在这里将能够进行地理感知并分组的地理分布共识算法表示为一组集群$C_i$的集合$S$。对每个集群$C_i$，都包含$\mathbf{n}$个副本，其中最多有$\mathbf{f}$个故障副本，并且$\mathbf{n} > 3\mathbf{f}$。

地理分布共识算法采用和Steward相同的故障模型，在系统$S$中最多可容纳$\mathbf{fz}$个故障副本，并且每个集群中的故障副本不超过$\mathbf{f}$个。这与Pbft等拜占庭容错共识算法不同。在这些算法中，都最多可容纳$\lfloor \mathbf{zn}/3 \rfloor = \lfloor \mathbf{z} (3\mathbf{f}+j)/3 \rfloor = \mathbf{fz}+\lfloor \mathbf{z}j/3 \rfloor$个副本。假设$\mathbf{n}=16$，$\mathbf{f}=5$，$\mathbf{z}=6$，那么地理分布共识算法可容纳$\mathbf{fz}=30$个故障副本，而Pbft可容纳$\lfloor \mathbf{zn}/3 \rfloor = 32$个故障副本。

下面定义地理分布共识算法所提供的共识：

\definition{
设S是R上的一个系统，任何共识协议的单次运行都需要满足以下两个要求：

可用性：R中每个副本都执行一个事务。

一致性：所有无故障副本执行相同的事务。
}

\section{本地复制}

地理分布共识算法的第一步是本地复制阶段。在这个阶段，每个集群都会独立的从本地客户端中选择一个请求去执行。在本地复制阶段，需要依赖于实用拜占庭容错共识协议Pbft。Pbft是一种三阶段共识协议，通过主节点$P_C$协调并达成一致性共识决策，在图\ref{figure:pbft}描述了一轮Pbft共识协议的过程。下面详细介绍这几个步骤。

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{source/pbft.png}
    \bicaption{\centering 单个集群中一轮PBFT共识算法的步骤}{Steps in a round of Pbft within a cluster $C$}
    \label{figure:pbft}
\end{figure}

首先，主节点$P_C$接收来自客户端的请求$\langle T \rangle_c$，表示事务T由客户端$c \in clients(C)$签名。

然后，主节点$P_C$广播一条Pre-Prepare消息将该请求提议给所有副本以发起对该请求的复制。当一个副本节点接收到Pre-Prepare消息后，将会进入下一阶段。当集群中有至少$\mathbf{n} - 2\mathbf{f} > \mathbf{f}$个无故障副本接收到Pre-Prepare消息后，则对于该请求的提议成功。

当副本接收到一条Pre-Prepare消息后，副本会进入PREPARE阶段，并且通过广播一条支持Pre-Prepare消息m的Prepare消息作为对Pre-Prepare消息的响应。广播完之后，副本将会等待直到接收到$\mathbf{n}-\mathbf{f}$条支持消息m的Prepare消息。这表明系统中至少有$\mathbf{n} - 2\mathbf{f} > \mathbf{f}$个非故障副本支持消息m，此时将会进入下一阶段。

最后，接收到n-f条Prepare消息后，副本会进入COMMIT阶段，并且通过广播一条支持消息m的Commit消息。当副本接收到n-f条支持消息m的Commit消息后，能够保证最终所有的副本都可以提交请求$\langle T \rangle_c$。

在本地复制阶段的最后，每个无故障副本都能够构造一个提交证明$[\langle T \rangle_c,\rho]_R$来证明该次提交的确发生了。其中，提交证明$[\langle T \rangle_c,\rho]_R$由客户端请求$\langle T \rangle_c$和$\mathbf{n} - \mathbf{f} > 2\mathbf{f}$个相同的支持$\langle T \rangle_c$的Commit消息组成，并且这些Commit消息由不同的副本节点签名。

\lemma{
\cite{castro1999practical}\cite{castro2002practical} 设S是一个系统，并且$C \in S$是一个满足$\mathbf{n} > 3\mathbf{f}$的集群，那么有

可用性：如果存在可靠的有限延迟通信，并且一个副本$R \in C$能够构造一个提交证明$[\langle T \rangle_c,\rho]_R$，那么所有的无故障副本$R^{'} \in nf(C)$最终都能构造一个提交证明$[\langle T^{'} \rangle_{c^{'}}, \rho]_{R^{'}}$。

一致性：如果副本$R_1\text{，}R_2 \in C$能够分别构造提交证明$[\langle T_{1} \rangle_{c_{1}}, \rho]_{R_{1}}$和$[\langle T_{2} \rangle_{c_{2}}, \rho]_{R_{2}}$，那么有$T_1=T_2$且$C_1=C_2$。
} 

\Proof{
Pbft能够保证一致性成立。当存在可靠的有限延迟通信时，为了保证系统可用性，Pbft使用了视图更换和检查点技术。如果主副本出现故障，视图更换协议保证无故障副本能够发现主副本故障，并触发主副本更换直到找到无故障主副本。检查点协议则能够保证系统从故障和恶意行为中恢复到一致性状态，从而保证系统的可用性。
}

\section{全局共享}

当一个本地集群完成本地复制阶段都后，会进入全局共享阶段，与其他所有的集群共享在本地复制阶段提交的客户端请求和生成的提交证明$[\langle T \rangle_c, \rho]_C$。全局共享阶段需要在集群间通信，该阶段需要在保障可靠检测到发送方故障的能力的前提下，最小化集群间通信数量。

假设S是一个系统，其中包含两个集群$C_1 \text{、} C_2\in$S。在全局共享阶段，集群$C_1$的主节点$P_{C_1}$向集群$C_2$发送消息$m = (\langle T \rangle_c, [\langle T \rangle_c, \rho]_{C_{1}})$，消息m包含了在第$\rho$轮共识中本地集群提交的客户端请求和对于该请求的提交证明。为了能够保证能够检测到发送方故障并同时最小化通信，集群$C_1$需要向集群$C_2$发送$\mathbf{f}+1$条消息m。

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{source/globalsharing.png}
    \bicaption{\centering 全局共享阶段的两个子阶段}{Two sub-phases of the global sharing}
    \label{figure:global}
\end{figure}

如图\ref{figure:global}所示，全局共享阶段包含两个子阶段。首先是一个全局阶段，在这个阶段主节点$P_{C_1}$将消息m发送到集群$C_2$的$\mathbf{f}+1$个副本。然后是一个本地阶段，这个阶段每个接收到正确的消息m的副本将消息m广播给本地集群的所有副本。

\proposition{
设S是一个系统，$C_1 \text{、} C_2 \in S$是两个集群，$m = (\langle T \rangle_c, [\langle T \rangle_c, \rho]_{C_{1}})$表示集群$C_1$在全局共享阶段发送给集群$C_2$的消息。有：

可用性：如果主节点$P_{C_1}$无故障并且存在可靠的有限延迟通信，那么$C_2$中的每个副本最终都将会接收到消息m。

一致性：如果集群$C_2$中的副本接收到两条来自集群$C_1$的消息$m_1 = (\langle T_1 \rangle_{c_1}, [\langle T_{1} \rangle_{c_1}, \rho]_{C_{1}})$和$m_2 = (\langle T_2 \rangle_{c_2}, [\langle T_{2} \rangle_{c_2}, \rho]_{C_{1}})$，那么一定有$T_1=T_2$，$C_1=C_2$。
}

\Proof{
首先是可用性证明。如果主节点$P_{C_1}$无故障，那么集群$C_2$中的$\mathbf{f}+1$个副本将会接收到消息m。因为集群$C_2$中至多有$\mathbf{f}$个拜占庭副本，因此在接收到消息m的副本中至少有一个副本是无故障副本，该副本会将消息m转发给集群$C_2$中的所有副本，保证了系统的可用性。

然后是一致性证明。提交证明$[\langle T \rangle_c, \rho]_C$中包含了$\mathbf{n} - \mathbf{f} > 2\mathbf{f}$个副本签名的Commit消息，因此提交证明不可能被故障副本伪造，所以$C_2$中副本接收到的消息m的完整性能够很容易验证。而\textbf{引理3.1}排除了其他任何消息$m^{'} = (\langle T^{'} \rangle_{c^{'}}, [\langle T^{'} \rangle_{c^{'}}, \rho]_{C_{1}})$的存在，保证了系统的一致性。
}

\section{远程视图更换}

在以上的过程中，存在两种情况下集群$C_2$中的副本无法接收到$C_1$中的消息m：一种是$P_{C_1}$是故障副本并且没有发送消息m到集群$C_2$中的$\mathbf{f}+1$个副本；另一种是通信不可靠导致消息超出了最大时延或者丢失。在这两种情况中，都会触发集群$C_2$中的无故障副本启动远程视图更换以使得集群$C_1$替换主节点$P_{C_1}$。在图\ref{figure:viewchange}中给出了远程视图更换中的消息传递过程。

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.6]{source/viewchange.png}
    \bicaption{\centering 远程视图更换}{Remote view change}
    \label{figure:viewchange}
\end{figure}

以$C_1$中的主节点没有或无法向$C_2$中的副本发送消息m为例。首先，集群$C_2$中的副本需要能够检测到集群$C_1$中的故障。因此，在第$\rho$轮的开始，集群$C_2$中的每个副本R都为集群$C_1$设置一个定时器，并等待来自集群$C_1$的有效消息m。如果定时器超时并且R仍然没有接收到消息m，那么R检测到第$\rho$轮中集群$C_1$的故障。

当$R \in C_2$检测到集群$C_1$的故障后，R会在集群$C_2$中启动与其他副本就该故障达成一致的过程。R会广播消息$DRvc(C_1,\rho,v_1)$到集群$C_2$中的所有副本。然后，R等待接收到$\mathbf{n} - \mathbf{f} > 2\mathbf{f}$个不同副本签名的相同的$DRvc(C_1,\rho,v_1)$消息后，能够保证集群$C_2$中的副本就集群$C_1$有故障达成一致。最后，R向集群$C_1$中对应的副本$Q \in C_1$发送消息$\langle Rvc(C_1,\rho,v_1)\rangle_R$来强制更换集群$C_1$的主节点$P_{C_1}$。

如果其他副本$R^{'} \in C_2$接收到了集群$C_1$的消息m，则$R^{'}$会用消息m响应消息$DRvc(C_1,\rho,v_1)$。这使得R能够在无法就集群$C_1$有故障达成一致的情况下进行恢复。除此之外，如果$R^{'} \in C_2$没有接收到消息m并且定时器也没有超时，那么当$R^{'}$接收到$\mathbf{f}+1$条$DRvc(C_1,\rho,v_1)$消息时便能够认为集群$C_1$发生了故障，因为这$\mathbf{f}+1$条消息中至少有一条来自无故障副本，并且该副本成功的检测到了集群$C_1$故障。

当副本$Q \in C_1$接收到视图更换请求$\langle Rvc(C_1,\rho,v_1)\rangle_R$并经过验证后，$Q$会广播该消息给集群$C_1$中的所有副本。当副本Q接收到$\mathbf{f}+1$条不同副本签名的完全相同的$Rvc(C_1,\rho,v_1)$消息后，它可以确定至少有一条远程视图更换请求来自集群$C_2$中的一个无故障副本。当集群$C_1$中没有正在进行的本地视图更换以及这是第一次接收到来自于集群$C_2$的对视图$v_1$的更换请求时，Q会检测主节点$P_{C_1}$的故障。

设S是一个系统，$C_1 \text{、} C_2 \in S$是两个集群，$m = (\langle T \rangle_c, [\langle T \rangle_c, \rho]_{C_1})$表示第$\rho$轮$C_1$需要发送给$C_2$的消息。如果存在可靠的有限延迟通信，那么$C_2$中的每个副本要么接收到消息m，要么$C_1$执行本地视图更换。在第$\rho$轮，集群中的每个副本将会接收到一组z个消息的集合$\{ (\langle T_i \rangle_{c_i},[\langle T_i \rangle_{c_i},\rho]_{C_i}) \, | \,(1 \leqslant i \leqslant \mathbf{z}) \wedge (c_i \in clients(C_i)) \}$。这些集合都包含相同的客户端请求。

\section{排序与执行}

当副本选择了要执行的客户端请求并且接收到了其他集群的所有客户端请求，便可以进入最后一步：排序并执行这些客户端请求。

在本地复制阶段和全局共享阶段结束后，每个副本在第$\rho$轮会收到相同的含有z个客户端请求的集合$S_\rho=\{ \langle T_i \rangle_{c_i} \, | \,(1 \leqslant i \leqslant \mathbf{z}) \wedge (c_i \in clients(C_i)) \}$。地理分布共识算法可以在集群上使用预定义的顺序，例如按照$[T_1, \cdots ,T_2]$的顺序执行客户端请求。执行完成后，每个副本将会通知其本地客户端。当客户端收到$\mathbf{f}+1$个相同的响应之后，可以保证其中至少有一条来自于一个无故障副本。

设S是一组副本上的一个系统，S中的每个集群$C_i$都满足$\mathbf{n} > 3\mathbf{f}$。如果存在可靠的有限延迟通信，地理分布共识算法可以保证S中的每个无故障副本在一个轮次中执行相同的$\mathbf{z}$个事务。

\chapter{设计实现}

\section{使用环境}

为了测试地理分布共识算法的性能，使用Go语言对地理分布共识算法和实用拜占庭容错算法进行了模拟仿真。实验环境为Intel i5-8250U CPU和8GB内存，操作系统为64位Ubuntu 20.04.4 LTS，Go语言版本go1.17.9 linux/amd64。

\begin{table}[h]
    \centering
    \bicaption{符号表2}{Symbol table 2}
    \begin{tabular}{cc}
    \toprule
        符号 & 定义 \\
        \hline
        $\langle T \rangle_c$/$\langle REQUEST,o,t,c \rangle_c$ & 由客户端签名的请求消息 \\
        $\langle \langle PRE-PREPARE,v,n,d \rangle_{P_C}, m\rangle$ & 由主节点$P_C$签名的Pre-Prepare消息 \\
        $\langle PREPARE,v,n,d,i \rangle_i$ & 由节点$i$签名的Prepare消息 \\
        $\langle COMMIT,v,n,d,i \rangle_i$ & 由节点$i$签名的Commit消息 \\
        $[\langle T \rangle_c,\rho]_C$ & 集群$C$生成的提交证明 \\
        $DRvc(C_i,\rho,v)$ & 集群内就集群$C_i$故障达成一致的消息 \\
        $Rvc(C_i,\rho,v)$ & 远程视图更换请求 \\
        $\langle REPLY,v,t,c,i,r>_i$ & 由节点$i$签名的Reply消息 \\
        % $\langle CHECKPOINT,n,d,i \rangle_i$ & 由节点$i$签名的检查点消息 \\
        $\langle VIEW-CHANGE,v+1,n,c,p,i \rangle_i$ & 由节点$i$签名的视图更换请求 \\
        % $\langle VIEW-CHANGE,v+1,n,C,P,i \rangle_i$ 应该是这样
        % C 表示2f+1个有效的checkpoint的证明
        % P时包含若干个Pm的集合，Pm定义为对于编号大于n且已经在节点i preppared的消息m，Pm包含一个合法的preprepare消息（不包括对应的request msg）和2f个对应的由不同节点签名的preprare消息（v,n,d要一样）
        $\langle NEW-VIEW,v+1,V,O \rangle_{P_C}$ & 由主节点$P_C$签名的新视图消息 \\
        $o$ & 客户端发起的请求执行的操作 \\
        $t$ & 时间戳 \\
        $c$ & 客户端id \\
        $\rho$ & 当前轮次 \\
        $v$ & 当前视图号 \\
        $n$ & 请求序列号 \\
        $d$ & 消息m的摘要 \\
        $r$ & 请求的执行结果 \\
        $V$ & 主节点$P_C$发送和接收的视图更换消息集合 \\
        $O$ & 一组Pre-Prepare消息的集合 \\
    \bottomrule
    \end{tabular}
    \label{tab:符号表2}
\end{table}

% Receive $\langle REPLY, v, t, c, i, r \rangle_{R_i}$ from $R_i \in C_k$

\section{功能模块}

为了实现算法的仿真模拟，系统在1到6台机器上采用多进程方式部署，节点总数量根据不同的测试需求分别有16、28、40、52、60个。除此之外，节点之间的通信采用远程过程调用（Remote Procedure Call，RPC）实现。

RPC是一种计算机通信协议，它允许一台计算机上的程序调用另一台计算机上的子程序，而不需要额外的为这个交互过程编程。Golang中已经有官方提供的库，因此调用起来十分方便。

除此之外，Golang中的goroutine很适用于并发编程。协程是一种轻量级用户态线程，不存在上下文切换问题，因此效率比较高。Goalng中使用goroutine只需要函数前添加一个go关键字即可为该函数创建一个goroutine。在下述的流程中，每个\textbf{event}都可以看作一个goroutine。

表\ref{tab:符号表2}中是本节所使用的消息符号说明。

\subsection{客户端}

\begin{figure}[h]
    \centering
    \begin{myprotocol}
        \TITLE{Send Request}{Used by the client $c \in clients(C_k)$}
        \STATE Send Request $\langle T \rangle_c$ to $P_{C_k}$.
        \SPACE
        \TITLE{Receive Reply}{Used by the client $c \in clients(C_k)$}
        \EVENT{Receive $\langle REPLY, v, t, c, i, r \rangle_{R_i}$ from $R_i \in C_k$}
            \IF{$|\langle REPLY, v, t, c, i, r \rangle_{R_i}| = \mathbf{f}+1$}
                \STATE Statistics latency and throughput.
            \ENDIF
        \ENDEVENT
    \end{myprotocol}
    \bicaption{\centering 客户端流程}{Client process}
    \label{fig:client}
\end{figure}

客户端由图\ref{fig:client}中所描述的两个流程组成。客户端向本地集群的主节点发送请求并等待直到收到$\mathbf{f}+1$条对该请求的签名正确且结果相同的回复之后，才能够把$r$作为正确的执行结果。因为故障副本数不超过$\mathbf{f}$个，所以$\mathbf{f}+1$条一致的响应消息一定能够保证执行结果是正确的。

\subsection{服务端$\backslash$副本节点}

\begin{figure}[h]
    \centering
    \begin{myprotocol}
        \TITLE{Pre-Prepare Phase}{Used by the replica $P_{C_k}$ of $C_k$}
        \EVENT{$P_{C_k}$ receives $\langle T \rangle_c$ from $c \in clients(C_k)$}
            \STATE Broadcast $\langle \langle PRE-PREPARE,v,n,d \rangle_{P_C}, m\rangle$ to all replicas $R_i \in C_k$.
        \ENDEVENT
        \SPACE
        \TITLE{Prepare Phase}{Used by the replicas $R_i \in C_k$}
        \EVENT{Receive $\langle \langle PRE-PREPARE,v,n,d \rangle_{P_C}, m\rangle$ from $P_{C_k}$ of $C_k$}
            \STATE Broadcast $\langle PREPARE,v,n,d,i \rangle_i$ to all replicas $R_i \in C_k$.
        \ENDEVENT
        \SPACE
        \TITLE{Commit Phase}{Used by the replicas $R_i \in C_k$}
        \EVENT{Receive $\langle PREPARE,v,n,d,i \rangle_i$ from $R_i \in C_k$}
            \IF{$|\langle PREPARE,v,n,d,i \rangle_i| = \mathbf{n} - \mathbf{f} > 2\mathbf{f}$}
                \STATE Broadcast $\langle COMMIT,v,n,d,i \rangle_i$ to all replicas $R_i \in C_k$.
            \ENDIF
        \ENDEVENT
        \SPACE
        \TITLE{Generate Commit Certificate}{Used by the replica $P_{C_k}$ of $C_k$}
        \EVENT{Receive $\langle COMMIT,v,n,d,i \rangle_i$ from $R_i \in C_k$}
            \IF{$|\langle COMMIT,v,n,d,i \rangle_i| = \mathbf{n} - \mathbf{f} > 2\mathbf{f}$}
                \STATE Generate commit certificate $[\langle T \rangle_c,\rho]_C$.
            \ENDIF
        \ENDEVENT
    \end{myprotocol}
    \bicaption{\centering 副本节点流程（本地复制阶段）}{Replicas process (Local replication phase)}
    \label{fig:replicalocal}
\end{figure}

在副本节点的本地复制阶段包括图\ref{fig:replicalocal}中的四个流程。主节点在接收到本地客户端请求后，会为该消息分配一个序号$n$，然后广播Pre-Prepare消息并将客户端请求也广播到集群中。客户端请求本身可以不包含在Pre-Prepare消息中，这样可以使Pre-Prepare消息足够小，有利于优化消息传输的速率。当接收到Pre-Prepare消息后，集群中的副本进入准备阶段并广播Prepare消息。同时，副本会将接收到的Pre-Prepare消息和Prepare消息保存到消息日志。接收到足够的正确的Prepare消息后，副本进入提交阶段，并广播Commit消息到集群中并将其和接收到的Commit消息一并保存到消息日志中。

当Prepare消息经过验证并且接收到至少$2\mathbf{f}+1$个Commit消息（包括自己）和对应的Pre-prepare消息时，能够保证集群中的副本最终都会提交该客户端请求。此时，可以构造提交证明$[\langle T \rangle_c,\rho]_C$并进入下一阶段——全局共享阶段。

\begin{figure}[h]
    \centering
    \begin{myprotocol}
        \TITLE{Global Phase}{Used by the replica $P_{C_1}$ of $C_1$}
        \EVENT{Generated commit certificate $[\langle T \rangle_c,\rho]_C$}
            \STATE Choose $\mathbf{f}+1$ replicas in $C_2$.
            \STATE Send $m = (\langle T \rangle_c, [\langle T \rangle_c, \rho]_{C_{1}})$ to the $\mathbf{f}+1$ replicas in $C_2$.
        \ENDEVENT
        \SPACE
        \TITLE{Local Phase}{Used by the replicas $R_i \in C_2$}
        \EVENT{Receive $m = (\langle T \rangle_c, [\langle T \rangle_c, \rho]_{C_{1}})$ from $P_{C_1}$ of $C_1$}
            \STATE Broadcast $m = (\langle T \rangle_c, [\langle T \rangle_c, \rho]_{C_{1}})$ to all replicas $R_i \in C_2$.
        \ENDEVENT
    \end{myprotocol}
    \bicaption{\centering 副本节点流程（全局共享阶段）}{Replicas process (Global sharing phase)}
    \label{fig:replicaglobal}
\end{figure}

全局共享阶段由图\ref{fig:replicaglobal}中的两个子阶段构成。以两个集群之间的为例，假设存在两个集群$C_1$和$C_2$，集群$C_1$向$C_2$发送消息$m = (\langle T \rangle_c, [\langle T \rangle_c, \rho]_{C_{1}})$。在全局阶段，集群$C_1$的主节点$P_{C_1}$从集群$C_2$中选择$\mathbf{f}+1$个副本并发向它们送消息$m$，能够保证至少有一个无故障副本可以接收到消息$m$。当集群$C_2$中的副本接收到消息$m$后在集群$C_2$中进行广播。

\begin{figure}[h]
    \centering
    \begin{myprotocol}
        \TITLE{Reply to the client}{Used by the replica $R \in C_k$}
        \EVENT{Receive message $m = (\langle T \rangle_c, [\langle T \rangle_c, \rho]_{C_{i}})$}
            \IF{$|\{(\langle T_i \rangle_{c_i},[\langle T_i \rangle_{c_i}, \rho]_{C_i} | (1 \leqslant i \leqslant \mathbf{z}) \wedge (c_i \in clients(C_i))\}| = \mathbf{z}$}
                \STATE Broadcast $\langle REPLY,v,t,c,i,r>_i$ to local client $c \in clients(C_k)$.
            \ENDIF
        \ENDEVENT
    \end{myprotocol}
    \bicaption{\centering 副本节点流程（回复客户端）}{Replicas process (Reply to the client)}
    \label{fig:replicareply}
\end{figure}

当集群中的副本接收到一组$\mathbf{z}$个消息的集合$\{(\langle T_i \rangle_{c_i},[\langle T_i \rangle_{c_i}, \rho]_{C_i} | (1 \leqslant i \leqslant \mathbf{z}) \wedge (c_i \in clients(C_i))\}$后，副本将会向本地客户端发送请求的执行结果。注意，这里的副本仅向本地客户端发送结果（图\ref{fig:replicareply}）。

\subsection{远程视图更换}

远程视图更换包括图\ref{fig:replicaglobalviewchange}中的两个阶段。以两个集群为例，假设集群$C_1$是故障集群。如果集群$R \in C_2$检测到了集群$C_1$的故障，那么$R$在集群$C_2$中广播$\langle DRvc(C_i,\rho,v) \rangle_R$消息。当一个集群收到$\langle DRvc(C_i,\rho,v) \rangle_R$消息时，如果该节点在此前接收到了集群$C_1$正常传来的消息m，那么该节点用消息m响应消息$\langle DRvc(C_i,\rho,v) \rangle_R$；如果接收到了$\mathbf{f}+1$条$\langle DRvc(C_i,\rho,v) \rangle_R$消息并且自身没有探测到集群$C_1$故障，那么此时可认为集群$C_1$故障；如果接收到了$\mathbf{n}-\mathbf{f}$条$\langle DRvc(C_i,\rho,v) \rangle_R$消息，那么在集群$C_2$中就集群$C_1$故障达成一致，此时发送$\langle Rvc(C_i,\rho,v) \rangle_R$消息以强制更换集群$C_1$的主节点。

对于集群$C_1$，当其中的副本接收到$\langle Rvc(C_i,\rho,v) \rangle_R$消息时，在集群内广播该消息；当接收到来自集群$C_2$的$\mathbf{f}+1$条$\langle Rvc(C_i,\rho,v) \rangle_R$消息时，可认为主节点$P_{C_1}$故障并进行本地视图更换。

\begin{figure}[htbp]
    \centering
    \begin{myprotocol}
        \TITLE{Initiation Phase}{Used by the replica $R \in C_2$}
        \EVENT{Detect failure of $C_1$ in round $\rho$}
            \STATE Broadcast $\langle DRvc(C_i,\rho,v) \rangle_R$ to all replicas in $C_2$.
        \ENDEVENT
        \EVENT{Receive $\langle DRvc(C_i,\rho,v) \rangle_{R_i}$ from $R_i \in C_2$}
            \IF{$R$ received $m = (\langle T \rangle_c, [\langle T \rangle_c, \rho]_{C_{1}})$ from $Q \in C_1$}
                \STATE Send $m = (\langle T \rangle_c, [\langle T \rangle_c, \rho]_{C_{1}})$ to $R_i$.
            \ENDIF
            \IF{$|\{\langle DRvc(C_i,\rho,v) \rangle_{R_i} \, | \, 1\leqslant i \leqslant \mathbf{n}| = \mathbf{f}+1$}
                \STATE Detect failure of $C_1$.
            \ENDIF
            \IF{$|\{\langle DRvc(C_i,\rho,v) \rangle_{R_i} \, | \, 1\leqslant i \leqslant \mathbf{n}| = \mathbf{n}-\mathbf{f}$}
                \STATE Send $\langle Rvc(C_i,\rho,v) \rangle_R$ to $Q \in C_1$, id($Q$)=id($R$).
            \ENDIF
        \ENDEVENT
        \SPACE
        \TITLE{Response Phase}{Used by the replicas $Q \in C_1$}
        \EVENT{Receive $\langle Rvc(C_i,\rho,v) \rangle_{R_i}$ from $C_2$}
            \STATE Broadcast $\langle Rvc(C_i,\rho,v) \rangle_{R_i}$ to all replicas in $C_1$.
            \IF{$|\{\langle Rvc(C_i,\rho,v) \rangle_{R_i} | (1 \leqslant i \leqslant \mathbf{n}) \wedge (R_i \in C_2) \}|=\mathbf{f}+1$}
                \STATE Detect failure of $P_{C_1}$.
            \ENDIF
        \ENDEVENT
    \end{myprotocol}
    \bicaption{\centering 副本节点流程（远程视图更换阶段）}{Replicas process (Global view change phase)}
    \label{fig:replicaglobalviewchange}
\end{figure}

\begin{figure}[htbp]
    \centering
    \begin{myprotocol}
        \TITLE{Local View Change}{Used by the replica $R$ of $C_k$}
        \EVENT{Detect failure of $P_{C_k}$}
            \STATE $v \GETS v+1$.
            \STATE Broadcast $\langle VIEW-CHANGE,v+1,n,c,p,i \rangle_R$ to all replicas in $C_k$.
        \ENDEVENT
        \EVENT{Receive $\langle VIEW-CHANGE,v+1,n,c,p,i \rangle_R$ from $R \in C_k$}
            \IF{$|\langle VIEW-CHANGE,v+1,n,c,p,i \rangle_R|=2\mathbf{f}$}
                \STATE Broadcast $\langle NEW-VIEW,v+1,V,O \rangle_{P_C}$ to all replicas $R \in C_k$.
            \ENDIF
        \ENDEVENT
    \end{myprotocol}
    \bicaption{\centering 副本节点流程（本地视图更换阶段）}{Replicas process (Local view change phase)}
    \label{fig:replicalocalviewchange}
\end{figure}

\subsection{本地视图更换}

当本地集群中的某个副本为本地集群主节点设置的定时器超时，或者接收到足够且正确的远程视图更换阶段消息$\langle Rvc(C_i,\rho,v) \rangle_{R_i}$时，可以触发本地的视图更换。首先会在本地集群中广播视图更换消息$\langle VIEW-CHANGE,v+1,n,c,p,i \rangle_R$。当新视图中的主节点接收到$2\mathbf{f}$条有效的来自不同节点相同的视图更换消息时，会在本地集群中广播新视图消息$\langle NEW-VIEW,v+1,V,O \rangle_{P_C}$。副本节点接收到有效的新视图消息后，主节点和副本都将进入新视图$v+1$（图\ref{fig:replicalocalviewchange}）。

\chapter{测试评估}

为了对地理分布共识算法GeoBFT和实用拜占庭容错算法Pbft进行对比分析，首先使用Golang实现了两种算法。然后在腾讯云上进行了部署，使用了六台4核8GB内存内存的主机，每台主机上运行一个客户端进程和多个服务端副本节点进程。每次测试每个客户端都会发送1000条请求。为了模拟真实环境中的网络延迟，参考了现有的不同区域的网络延迟数据\cite{gupta2020resilientdb}并在配置文件中设置分属不同集群的副本节点之间的网络延迟。

\section{集群数量的影响}

%% num of clusters throughput
\pgfplotstableread{
clusters	pbft    geobft
1   17171	16346
2   17320	19161
3   16087	20071
4   12724	20884
5   9181	22440
6   8792	22468
}\dataTPUTfnCLUSTER

%% num of clusters latency
\pgfplotstableread{
clusters	pbft    geobft
1	0.666	0.694
2	0.612	0.730
3	0.641	0.745
4	1.280	0.805
5	1.807	0.820
6	2.066	0.906
}\dataLATfnCLUSTER

\begin{figure}[htbp]
    \begin{tikzpicture}[scale=0.8]
        \begin{axis}[xlabel=集群数量,
                        ylabel=吞吐量]
            \addplot table[x=clusters,y=pbft] {\dataTPUTfnCLUSTER};
            \addplot table[x=clusters,y=geobft] {\dataTPUTfnCLUSTER};
        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[scale=0.8]
        \begin{axis}[xlabel=集群数量,
                        ylabel=延迟,
                        legend pos=outer north east]
            \addplot table[x=clusters,y=pbft] {\dataLATfnCLUSTER};
            \addplot table[x=clusters,y=geobft] {\dataLATfnCLUSTER};
            \legend{Pbft,Geobft};
        \end{axis}
    \end{tikzpicture}
    \bicaption{\centering 集群数量的影响}{Impact of the number of clusters}
    \label{fig:numofclusters}
\end{figure}

为了测试集群数量对于性能的影响，首先保证系统中副本总数不变，集群数量从1增加到6。在实验过程中，对于每种情况下的数据，按相同配置运行三次取平均值作为最终的结果。在两种算法中，集群数量对于系统性能的影响如图\ref{fig:numofclusters}所示。

可以看到当集群数量较少的时候，Pbft能够维持比较好的性能。在单个集群上运行时，地理分布共识算法由于全局共享阶段的存在造成额外开销，所以性能相对比较低一点。随着集群数量的增多，全局通信成为系统性能的瓶颈，此时Pbft算法的性能快速下降，而地理分布共识算法能够实现较好的扩展性。这是因为相对于Pbft算法中不论本地或者全局，都需要副本之间的两两通信，而地理分布共识算法由于在集群间只需要传递$\mathbf{f}+1$条消息，远少于Pbft所需的全局间通信数量。因此，地理分布共识算法的性能大约是Pbft的2.5倍。

\section{集群中副本数量的影响}

%% num of replicas throughput
\pgfplotstableread{
replicas	pbft    geobft
4	15572 	28846 
7	14862 	26754 
10	14387 	24364 
13	13984 	22663 
16	13422 	21431 
}\dataTPUTfnREPLICA

%% num of replicas latency
\pgfplotstableread{
replicas	pbft    geobft
4	0.583	0.509
7	0.619	0.601
10	0.644	0.634
13	0.744	0.612
16	0.928	0.844
}\dataLATfnREPLICA

\begin{figure}[htbp]
    \begin{tikzpicture}[scale=0.8]
        \begin{axis}[xlabel=副本数量,
                        ylabel=吞吐量]
            \addplot table[x=replicas,y=pbft] {\dataTPUTfnREPLICA};
            \addplot table[x=replicas,y=geobft] {\dataTPUTfnREPLICA};
        \end{axis}
    \end{tikzpicture}
    \begin{tikzpicture}[scale=0.8]
        \begin{axis}[xlabel=副本数量,
                        ylabel=延迟,
                        legend pos=outer north east]
            \addplot table[x=replicas,y=pbft] {\dataLATfnREPLICA};
            \addplot table[x=replicas,y=geobft] {\dataLATfnREPLICA};
            \legend{Pbft,Geobft};
        \end{axis}
    \end{tikzpicture}
    \bicaption{\centering 集群中副本数量的影响}{Impact of the number of replicas in the cluster}
    \label{fig:numofreplicas}
\end{figure}

每个集群中副本数量对于性能的影响如图\ref{fig:numofreplicas}所示。为了测量集群中副本数量对于性能的影响，需要保证集群总数不变，集群中副本的数量分别选取$\mathbf{f}=1$，$2$，$3$，$4$，$5$，所以单个集群中副本数量$\mathbf{n}=3\mathbf{f}+1=4$，$7$，$10$，$13$，$16$。

可以看到，增加集群中的副本数量对于Pbft的影响并不是很大。Pbft中的通信瓶颈在于主节点与其他集群的单个副本节点之间的远程通信速度哦，仅仅增加副本数量并不会改变远程通信速度，其性能的下降是由于系统中副本节点增多带来的通信次数增多导致。而地理分布共识算法中，随着单个集群中副本节点的增多，在全局共享阶段需要构造的提交证明也会变大。因此会造成全局共享阶段本地主节点与其他集群间副本节点的消息传递时间增长，从而导致系统的性能下降。

\section{批大小的影响}

%% batch size
\pgfplotstableread{
batchsize	pbft    geobft
10	9511	12327
50	16251	20608
100	17241	29580
200	17916	36763
300	18755	39319
}\dataBATCHSIZE

\begin{figure}[htbp]
    \centering
    \begin{tikzpicture}[scale=0.8]
        \begin{axis}[xlabel=批大小,
                        ylabel=吞吐量,
                        legend pos=outer north east]
            \addplot table[x=batchsize,y=pbft] {\dataBATCHSIZE};
            \addplot table[x=batchsize,y=geobft] {\dataBATCHSIZE};
            \legend{Pbft,Geobft};
        \end{axis}
    \end{tikzpicture}
    \bicaption{\centering 批大小的影响}{Impact of Batch size}
    \label{fig:batchsize}
\end{figure}

批大小定义为单次共识决策中处理的客户端请求的数量。为了测量批大小对性能的影响，需要保证集群数量和单个集群中的副本数量不变。批大小分别取10，50，100，200和300，测量结果如图\ref{fig:batchsize}所示。

由于系统中集群数量与单个集群中的副本数量都是固定的，因此系统通信复杂度也是固定的，只需要考虑批大小对于吞吐量的影响即可，测量如图\ref{fig:batchsize}所示。在Pbft中，由于存在中心化的主节点，系统的带宽限制在该单一主节点上。随着批大小的增大，主节点带宽成为系统性能瓶颈。而地理分布共识算法中，每个集群内都有本地主节点，在整个系统中分散了共识，消除了单个主节点带宽的限制，从而能够支持更大的批，性能约为Pbft的两倍。

\chapter{总结展望}

本文实现并对比分析了地理分布共识算法和实用拜占庭容错共识算法的性能。地理分布共识算法通过利用网络拓扑关系，将本地副本分组到同一个集群中，通过分散共识消除了实用拜占庭容错共识算法中由于单个主节点集中式协调共识决策造成的性能瓶颈。除此之外，在保证能够可靠探测到故障的情况下，最大限度的减少了全局间通信。通过对比地理分布共识算法和Pbft共识算法在几个方面的性能，可以看出，地理分布共识算法拥有比Pbft更高的性能。因此，地理分布共识算法能够支持区块链系统的跨地理规模部署。尽管如此，地理分布共识算法中仍然有可以改进的地方。在全局共享阶段，所生成的提交证明的大小会影响该阶段的系统性能，在下一阶段可以尝试使用阈值签名\cite{shoup2000practical}通过生成单个恒定大小的阈值签名来解决该问题。另一方面，在利用网络拓扑信息划分集群上，如何更合理的将节点划分到一个集群中，能够更好的提升系统的性能也有待展开研究。除此之外，包括地理分布共识算法在内的拜占庭容错算法目前都仅支持在许可区块链系统中使用，如何让其也能够在非许可链中使用也是未来的一个研究方向。

% 正文后部分
\backmatter
% 导入参考文献 (需要通过 latexmk 编译后才能显示)
\printbibliography

% 新增：导入未被引用的参考文献 (需要通过 latexmk 编译后才能显示)
\nocite{cite_label2}
\nocite{cite_label3}


% 附录环境
\begin{appendix}

\end{appendix}



% 致谢环境
\begin{acknowledgement}

五年时间，转眼间匆匆而过。

犹记得2017年6月8号下午考完英语，收拾好东西坐在回家的车上，看着校园里熙熙攘攘的人群，一个个大包小包提着东西，就此别离，突然间流下眼泪来，吓得我妈以为我没考好赶紧安慰我。那时的高中校园，高一高二的校园，也是如华师大这般，郁郁葱葱。高三的校园虽没了绿树成荫，但回想起高三，浮现的总是阳光洒落在走廊里背书的同学身上的画面。

对于在这里生活了五年之久的华师大，也有着一些自己与她的独特联系吧。文史楼前消失的那棵树，丽娃停车场角落里不见了的海宝，大活旁边的小假山，丽虹桥底系着的小船，那只蹭了我一裤子毛的三花，以及为数不多但很漂亮的槭树，当然，还有华师大里可爱的人们。我不长于交际，更多的时候喜欢坐在旁边，坐在远处，看着人来人往。但，偶尔又会离开旁观者的位置，在另一个毕业季的晚上，与打印店小哥和一个俄罗斯女生畅谈到深夜。细细想来，平淡枯燥在期末赶ddl的生活中也不乏一些回想起来嘴角上扬的片段。不过，我现在最想做的，是能够到草坪上打滚！

好了，回到正题。对于即将完成的毕业论文，十分感谢在选题，开题，实验和写作中给予帮助的范维学长和张召老师。范维学长虽然有一点shy，但是喜欢跑步和邓丽君的男生，应该自有一方天地吧！有一位豆友，也喜欢邓丽君，是一位有着自己人格魅力的低调自律不张扬有血有肉有温度的男生。张召老师是一位认真负责的老师，在疫情初始网课期间由于一些生活问题也和张召老师谈过几次话，而后在考研选择研究生导师的谈话中也能体会到张召老师的关心。当然，我们院的每位老师都很关心同学们的学习以及日常生活！在这里还要感谢一下辅导员杨兴龙老师，在疫情隔离期间麻烦了杨老师好多事情，谢谢杨老师！除此之外，还要感谢孙老师（感觉称呼孙秋实同学似乎有点遥远的距离感，还是写孙老师吧哈哈哈）提供的 \LaTeX 模板，让我能够免于Word调格式的痛苦，感谢！

除了毕业论文之外，由于疫情封校的原故，在准备考研复试环境上也是一波三折。因此，十分感谢借给我电脑用的粱辉哥和吴佳威学妹以及借给我手机支架的于佩民小朋友，感谢他们让我能够顺利完成考研复试！

五年的时间里，在这里认识了不少的同学，也有几位好友可以时不时出来一起吃个饭。希望已经毕业的他们不要受到疫情的过多干扰，也希望疫情能够早点过去，恢复疫情前的生活状态！

现在的我，与五年前的我，总归是不太一样的。对于这五年来的学习生活时光，不能说很满意，也不算太差。前几天看到一本书的书评中有一句话：“……为荒废过许多青春时间而悲哀。但还好学无止境，受用于此书，自当勤勉”。

学无止境，受用于此，自当勤勉。
\\

\rightline{二〇二二年四月二十三日，华东师范大学中北三馆}

\end{acknowledgement}

\end{document}